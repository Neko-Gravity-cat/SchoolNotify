#for page-based crawling

page_to_crawl = 1

for i in range(1, page_to_crawl + 1):
  header_ = {
    "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36"
  } #pretend to be a browser
  response = requests.get(f"http://www.{os.environ['host']}/files/501-1000-1001-{i}.php?Lang=zh-tw", headers = header_)

  response.encoding = response.apparent_encoding

  soup = BeautifulSoup(response.text, "html.parser")
  soup.encoding = response.encoding

  row = soup.find_all("tr", class_ = ["row_01", "row_02"])

  for r in row:
    #hyperlink of the article
    division = r.select_one("div")
    anchor = division.select_one("a")

    link = anchor["href"]

    #title, source (unwanted), and date
    table = r.select("td")
    table_list = []

    for td in table:
      table_list.append(td)

    title = table_list[0].text.strip()
    #source = table_list[1].text.strip() #unwanted
    date = time.strptime(table_list[2].text.strip(), "%Y-%m-%d")
    result.append(Announcement(link, title, date))
